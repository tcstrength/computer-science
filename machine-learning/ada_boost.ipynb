{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "AdaBoost, short for Adaptive Boosting, is a machine learning algorithm that is commonly used for classification tasks. It belongs to the family of ensemble learning methods, which combine the predictions of multiple individual models (called weak learners) to create a more accurate and robust final prediction.\n",
    "\n",
    "### Algorithm\n",
    "The basic idea behind AdaBoost is to sequentially train a series of weak learners on different subsets of the training data. Each weak learner is trained to focus on the examples that were misclassified by the previous weak learners, thereby attempting to correct the mistakes made by the previous models. During each iteration of training, the algorithm assigns weights to the training examples, with higher weights given to the examples that were misclassified in the previous iteration.\n",
    "\n",
    "#### Training AdaBoost\n",
    "0. Initialize weight $w_t = 1 / N$ where $N$ is number of training samples \n",
    "1. Train weak learner $h_t$ on given sample weights (time step $t$)\n",
    "2. Calculate the error $\\epsilon_t = \\sum_{h_t(x_i) \\ne y_i} w_{t,i}$\n",
    "3. Find the alpha $\\alpha_t$ of $h_t$, $\\alpha_t = 0.5 * ln((1 - \\epsilon_t)/\\epsilon_t)$\n",
    "4. Reassign new weight $w_{t+1, i} = w_{t,i} * e^{\\alpha_t * h_t(x_i) * y_i}$\n",
    "5. Normalize the weight to make sure sum up to 1, $w_{t+1, i} = w_{t+1, i} / \\sum_j w_{t+1, j}$\n",
    "\n",
    "#### Predict\n",
    "Assume we already trained the AdaBoost of `K` estimators:\n",
    "$H(x) = sign(\\sum_t^K \\alpha_t * h_t(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_hastie_10_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_hastie_10_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_boost(X_train, Y_train, n_estimators: int):\n",
    "    weights = np.ones_like(Y_train) / len(Y_train)\n",
    "    alphas = np.zeros(n_estimators)\n",
    "    clfs = []\n",
    "\n",
    "    for i in range(n_estimators):\n",
    "        wl = DecisionTreeClassifier(max_depth=2, max_features=\"log2\")\n",
    "        wl.fit(X_train, Y_train, weights)\n",
    "\n",
    "        Y_pred = wl.predict(X_train)\n",
    "        error = np.sum(weights[Y_pred != Y_train])\n",
    "\n",
    "        if error > 0.5:\n",
    "            print(f\"No improvement, stop at iteration [{i}]!\")\n",
    "            return bag_of_wl[:i], alphas[:i]\n",
    "\n",
    "        \n",
    "        alphas[i] = 0.5 * np.log((1 - error) / error)\n",
    "        weights *= np.exp(-alphas[i] * Y_pred * Y_train)\n",
    "        weights /= np.sum(weights)\n",
    "        clfs.append(wl)\n",
    "\n",
    "    return clfs, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_predict(clfs: list, alphas: list, X):\n",
    "    stack = []\n",
    "    for i, h in enumerate(clfs):\n",
    "        p = alphas[i] * h.predict(X)\n",
    "        stack.append(p)\n",
    "    Y_pred = np.array(stack)\n",
    "    return np.sign(Y_pred.sum(axis=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train: 0.8703333333333333\n",
      "Accuracy on test: 0.8446666666666667\n"
     ]
    }
   ],
   "source": [
    "clfs, alphas = ada_boost(X_train, Y_train, 50)\n",
    "\n",
    "Y_pred = ada_predict(clfs, alphas, X_train)\n",
    "print(f\"Accuracy on train: {np.sum(Y_pred == Y_train) / len(Y_train)}\")\n",
    "\n",
    "Y_pred = ada_predict(clfs, alphas, X_test)\n",
    "print(f\"Accuracy on test: {np.sum(Y_pred == Y_test) / len(Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train: 0.952\n",
      "Accuracy on test: 0.9233333333333333\n"
     ]
    }
   ],
   "source": [
    "clfs, alphas = ada_boost(X_train, Y_train, 200)\n",
    "\n",
    "Y_pred = ada_predict(clfs, alphas, X_train)\n",
    "print(f\"Accuracy on train: {np.sum(Y_pred == Y_train) / len(Y_train)}\")\n",
    "\n",
    "Y_pred = ada_predict(clfs, alphas, X_test)\n",
    "print(f\"Accuracy on test: {np.sum(Y_pred == Y_test) / len(Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train: 0.974\n",
      "Accuracy on test: 0.9398333333333333\n"
     ]
    }
   ],
   "source": [
    "clfs, alphas = ada_boost(X_train, Y_train, 400)\n",
    "\n",
    "Y_pred = ada_predict(clfs, alphas, X_train)\n",
    "print(f\"Accuracy on train: {np.sum(Y_pred == Y_train) / len(Y_train)}\")\n",
    "\n",
    "Y_pred = ada_predict(clfs, alphas, X_test)\n",
    "print(f\"Accuracy on test: {np.sum(Y_pred == Y_test) / len(Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
