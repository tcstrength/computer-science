{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4874b01b-d1ce-4bf2-a858-c534ed9fb890",
   "metadata": {},
   "source": [
    "### Bayes' Rule\n",
    "From Wikipedia, \"describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if the risk of developing health problems is known to increase with age, Bayes' theorem allows the risk to an individual of a known age to be assessed more accurately by conditioning it relative to their age, rather than assuming that the individual is typical of the population as a whole.\"\n",
    "\n",
    "$$P(A|B) = \\frac{P(A) P(B|A)}{P(B)}$$\n",
    "\n",
    "- The $P(A|B)$ is the posterior\n",
    "- The $P(A)$ is the prior\n",
    "- The $P(B|A)$ is the likelihood\n",
    "- The $P(B)$ is the evidence\n",
    "\n",
    "### Naive Asumption\n",
    "The naive part comes from the assumption that all features are independent which are not always true in real world problems but this assumption works quite well in practice. From an unknown joint distribution $(X_1,X_2,...,X_n)$, the posterior of Y takes class c:\n",
    "$$P(Y=c|X_1, X_2,..., X_n) = \\frac{P(X_1,X_2,...,X_n|Y=c) P(Y=c)}{P(X_1,X_2,...,X_n)}$$\n",
    "\n",
    "Because of the naive assumption, the formula will be:\n",
    "$$P(Y=c|X_1, X_2,..., X_n) = \\Pi \\frac{P(X_i|Y=c)P(Y=c)}{P(X_i)}$$\n",
    "\n",
    "For $c \\in C$, the prediction will be the one that has highest posteriori probability:\n",
    "$$\\hat{y} = \\underset{c}{\\mathrm{argmax}} \\; P(X|y=c)P(Y=c)$$\n",
    "\n",
    "### Hidden Markov Model\n",
    "Another assumption that we can use to solve the posterior is to make an assumption that each variable X_1 depends on X_2, X_2 depends on X_3, ... Our problem will turns into Hidden Markov Chain where the state is the target class and the outputs of the chain are X1, X2, X3,... X_n\n",
    "\n",
    "__Todo:__ Check the feasibility and implement\n",
    "\n",
    "### Likelihood of Features\n",
    "#### a) Likelihood as Gaussian\n",
    "The likelihood of features is assumed to be __Gaussian__, the formula will be:\n",
    "$$P(X|y=c) = P(X|\\mu_{y=c}, \\sigma_{y=c}) P(\\mu = \\mu_{y=c}, \\sigma = \\sigma_{y=c})% = \\frac{1}{\\sqrt{2 \\pi \\sigma_{y=c}}} exp(-\\frac{(X - \\mu_{y=c})^2}{2 \\sigma^2_{y=c}})$$\n",
    "\n",
    "However, we do not have any prior knowledge about the parameter $\\theta(\\mu, \\sigma)$, we assume that the distributions of $\\mu$ and $\\sigma$ are uniform, and therefore the probability stay unchange, it will be:\n",
    "$$P(X|y=c) = P(X|\\mu_{y=c}, \\sigma_{y=c})% = \\frac{1}{\\sqrt{2 \\pi \\sigma_{y=c}}} exp(-\\frac{(X - \\mu_{y=c})^2}{2 \\sigma^2_{y=c}})$$\n",
    "\n",
    "\n",
    "#### b) Likelihood as Categorical Distribution\n",
    "Each feature is assumed to have its own __categorical__ distribution, or given any class $y=c$, the probability that X takes the value x:\n",
    "$$P(X=x|y=c) = \\frac{|\\{ i \\in D | X=x, y=c \\} + \\alpha|}{ |\\{ i \\in D | y = c \\}| + \\alpha C}$$\n",
    "__Note:__ $\\alpha$ is the smoothing parameter to prevent zero probability "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465d5ad7-6a17-401d-ad88-265644a6c9d1",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "94c9dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eb2d7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e0ebd2",
   "metadata": {},
   "source": [
    "#### Using Likelihood as Gaussian\n",
    "- Use MLE to estimate the $\\theta$ and $\\mu$\n",
    "- $\\mu = \\frac{1}{n} \\sum_i^n x_i$\n",
    "- $\\sigma = \\sqrt{\\frac{\\sum_i^n (x_i - \\mu)^2}{n}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7092d961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.2039711007793663, 17.462830188679245)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_theta(X, y, cls):\n",
    "    T = X[y == cls]\n",
    "    m = T.mean().to_frame(\"u\")\n",
    "    s = T.std().to_frame(\"s\")\n",
    "    return m.join(s).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "18351206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_log_proba(X, thetas):\n",
    "    X = X.copy()\n",
    "    cols = X.columns\n",
    "    def _udf(x, cols, params):\n",
    "        # return stats.norm.pdf(x, loc=m, scale=s)\n",
    "        log_proba = 0\n",
    "        for col in cols:\n",
    "            u, s = params[\"u\"][col], params[\"s\"][col]\n",
    "            p = stats.norm.pdf(x[col], loc=u, scale=s)\n",
    "            if p == 0:\n",
    "                continue\n",
    "            log_proba += np.log(p)\n",
    "        return log_proba\n",
    "    \n",
    "    for cls in thetas:\n",
    "        params = thetas[cls]\n",
    "        X[f\"class_{cls}\"] = X.apply(lambda x: _udf(x, cols, params), axis=1)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64e808c",
   "metadata": {},
   "source": [
    "#### Our GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b967d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = {\n",
    "    0: compute_theta(X_train, y_train, cls=0),\n",
    "    1: compute_theta(X_train, y_train, cls=1)\n",
    "}\n",
    "\n",
    "thetas[0][\"s\"][\"mean radius\"], thetas[0][\"u\"][\"mean radius\"]\n",
    "X_out = predict_log_proba(X_test, thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "045155da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "y_pred_imp = np.argmax(X_out[[\"class_0\", \"class_1\"]], axis=1)\n",
    "print(\"Accuracy:\", np.sum(y_pred_imp == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca7a0b",
   "metadata": {},
   "source": [
    "#### Scikit-Learn GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "db887b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_sk = clf.predict(X_test)\n",
    "print(\"Accuracy:\", np.sum(y_pred_sk == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4354d5",
   "metadata": {},
   "source": [
    "#### Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bd061dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same predictions 0.9912280701754386\n"
     ]
    }
   ],
   "source": [
    "print(\"Same predictions\", np.sum(y_pred_imp == y_pred_sk) / len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
